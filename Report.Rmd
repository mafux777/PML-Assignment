---
title       : Assignment "Practical Machine Learning"
subtitle    : Analysing Sensor Data
author      : Matthias Funke
---

```{r , echo=FALSE, results='hide'}
## Practical Machine Learning Assignment
## Quality of Dumbell Exercises

load <- function(pkg){
        new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
        if (length(new.pkg))
                install.packages(new.pkg, dependencies = TRUE)
        sapply(pkg, require, character.only = TRUE)
} 

sumna = function (x){
        sum(is.na(x))
}

load(c("data.table", "caret", "ggplot2", "doParallel"))

cl <- makeCluster(detectCores())
registerDoParallel(cl)
```
## Synopsis
The goal of this assignment is to demonstrate how machine learning, in this case a random forest algorithm, can be used to predict whether a particular exercise was done "correctly". The way that the experiment was set up is that the correct way was called "Classe A" and there were 4 ways to do the job incorrectly, "Classe B..E". We trained the RF model with 75% of the data and checked the out-of-sample error by letting the model predict on the remaining 25% test case to obtain an accuracy of 98%. This was done purely on data from accelerometers, gyros, and magnetos. Data for time, window, user, sequence number, were all suppressed. 
Apparently other students were able to obtain higher accuracy by including user name as a predictor, but I chose not to use it to keep the model more "honest".

## Procedure
Read data from CSV file, count NA values and in general explore the data. Eliminate columns with NA data and show how many predictors are left, using nearZeroVar utility function to show variability. Note how all the remaining sensor data predictors have enough variability, therefore the output is informational only. 
```{r}
setwd("/Users/mfunke/Downloads")
data=read.csv("pml-training.csv", dec='.',na.strings=c("#DIV/0!", "NA"), stringsAsFactors=F)
data=data.table(data)
# identify columns without NA values --- sumna is a utility function defined as sum(is.na(x))
good_vars=data[,lapply(data, sumna)==0]
# select only the columns where we have data
data2=data[, good_vars, with=F]
```
```{r, results='hide'}
data2[, classe:=as.factor(classe)] # crucial otherwise the model crashes
```
```{r}

## Pre-process: create training partitions and test partition
inTrain = as.vector(createDataPartition(y=data2$classe, p=.75, list=F))
training=data2[inTrain]
testing=data2[-inTrain]

nearZeroVar(training, saveMetrics=T)
```
As discussed, we want to use only data from the sensors (and Euler derived pitch, roll), so grep for the appropriate variable names. Then, use
principal component analysis to reduce the number of predictors to relevant set of 23. 
```{r}
g=grep("(^roll)|(^accel)|(^gyros)|(^magnet)|(^pitch)", names(data2))

p2=preProcess(training[,g,with=F], method="pca")
trainPC=predict(p2, training[,g,with=F])
testPC=predict(p2, testing[,g,with=F])
```
Now, run the model "rf" (random forest) and see how it does on the training set. 
```{r}
c=trainControl(method="repeatedcv")
modelFit=train(training$classe ~ ., method="rf", data=trainPC, trControl=c)
modelFit
confusionMatrix(testing$classe, predict(modelFit, testPC))
```
The confusion matrix gives me a good estimate of the out-of-sample error, because it uses data that it was not trained on to predict a set of values with the correct answer known. This is called cross-validation. (The model itself already did cross-validation automatically.)
```{r}
setwd("/Users/mfunke/Downloads")
realtest=read.csv("pml-testing.csv", dec='.',na.strings=c("#DIV/0!", "NA"), stringsAsFactors=F)
realtest=data.table(realtest)
# select only the columns where we have data
realtest=realtest[, good_vars, with=F]
realPC=predict(p2, realtest[,g,with=F])
answers=predict(modelFit, realPC)
answers
```
Finally, we have run the model on the test data provided as part of the assignment and submitted. On the first attempt, got 18/20 correct. It would appear that the accuracy in this instance (90%) is a bit lower than what we estimated above, and I don't have an explanation for that right now. We don't know how the final test set was selected. 